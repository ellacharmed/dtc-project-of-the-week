{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework for week03\n",
    "\n",
    "> We'll keep working with the MSRP variable, and we'll transform it to a classification task.\n",
    "\n",
    "> In this homework, we will use the Car price dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uri = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget $uri -O data-hmwk-3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/data-hmwk-3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive analytics - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{data.shape =}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.replace(' ', '_').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.replace('msrp', 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['make', 'model', 'year',  'engine_hp', 'engine_cylinders', \n",
    "       'transmission_type','vehicle_style',  'highway_mpg', 'city_mpg']\n",
    "target = ['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = list(data[features].dtypes[data.dtypes == 'object'].index)\n",
    "numerical_columns = list(data[features+target].dtypes[data.dtypes != 'object'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numerical_columns].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.box(figsize=(12, 12), layout=(3,3), subplots=True, sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(data\n",
    "                                    , title=\"Pandas Profiling Report\"\n",
    "                                    ,  config_file=\"../config_default.yaml\"\n",
    "                                    , explorative=True\n",
    "                                )\n",
    "\n",
    "profile.to_notebook_iframe()\n",
    "# profile.to_file(\"ProfileReport.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[features+target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "q: What is the most frequent observation (mode) for the column `transmission_type`?\n",
    "\n",
    "a: `AUTOMATIC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.transmission_type.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Create the correlation matrix for the numerical features of your dataset. In a correlation matrix, you compute the correlation coefficient between every pair of features in the dataset.\n",
    "\n",
    "Q: What are the two features that have the biggest correlation in this dataset?\n",
    "\n",
    "A: `highway_mpg` and `city_mpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data[numerical_columns].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(corr, text_auto=\".2f\", aspect=\"auto\", color_continuous_scale='RdBu_r')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make price binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean for price column\n",
    "mean_price = data.price.mean()\n",
    "# create a new column called 'above_average' that is set to 1 if value is above the mean_price\n",
    "data['above_average'] = [1 if price > mean_price else 0 for price in data['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.above_average.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train, df_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_full_train, hue=\"above_average\",  diag_kind=\"hist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.above_average.values\n",
    "y_val = df_val.above_average.values\n",
    "y_test = df_test.above_average.values\n",
    "\n",
    "df_train.drop(['price', 'above_average'], axis=1, inplace=True)\n",
    "df_val.drop(['price', 'above_average'], axis=1, inplace=True)\n",
    "df_test.drop(['price', 'above_average'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "- Calculate the mutual information score between above_average and other categorical variables in our dataset. Use the training set only.\n",
    "- Round the scores to 2 decimals using round(score, 2).\n",
    "\n",
    "\n",
    "Q: Which of these variables has the lowest mutual information score?\n",
    "\n",
    "A: `transmission_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info_target(series):\n",
    "    return mutual_info_score(series, df_full_train.above_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = df_full_train[categorical_columns].apply(mutual_info_target)\n",
    "mi.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does having lowest `mi` score mean?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Q: What accuracy did you get?\n",
    "\n",
    "A: `0.94`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns.remove('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = df_train.to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "val_dict = df_val.to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', C=10, max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "y_pred_proba = model.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `.predict_proba` returns the probability of \n",
    "- (0, positive answer) `above_average` >= threshold and \n",
    "- (1, negative answer) `above_average` < threshold\n",
    "for each X_val observation, for threshold=0.5\n",
    "\n",
    "So, that's why we are only intrested in column [1], the negative answer aka the yes, churn equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict_proba(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_decision = (y_pred_proba >= 0.5)\n",
    "(y_val == pred_decision).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "round(accuracy_score(y_val, y_pred), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Q: Which of following feature has the smallest difference?\n",
    "\n",
    "A: `city_mpg`\n",
    "\n",
    "> [!Note] \n",
    "> :memo: I got the feature with smallest difference as `vehicle_style` with 0.04% difference as opposed to `city_mpg` with 0.21% (next smallest difference), but selected the smallest out of the MCQ options provided. \n",
    "> \n",
    "> :warning: Is my logic wrong, since my resulting feature is not in the options listed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# load the data\n",
    "data = pd.read_csv('../data/data-hmwk-3.csv')\n",
    "\n",
    "# identify features and target\n",
    "features = ['make', 'model', 'year', 'engine_hp', 'engine_cylinders', \n",
    "            'transmission_type', 'vehicle_style',\n",
    "            'highway_mpg', 'city_mpg']\n",
    "target = ['price']\n",
    "\n",
    "# clean/prepare the data\n",
    "data.columns = data.columns.str.replace(' ', '_').str.lower()\n",
    "data.columns = data.columns.str.replace('msrp', 'price')\n",
    "\n",
    "# use a subset for analysis\n",
    "data = data[features+target]\n",
    "\n",
    "# impute Nan/nulls with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "categoricals = list(data.dtypes[data.dtypes == 'object'].index)\n",
    "numericals = list(data.dtypes[data.dtypes != 'object'].index)\n",
    "\n",
    "\n",
    "# binarize target\n",
    "# calculate the mean for price column\n",
    "mean_price = data.price.mean()\n",
    "# create a new column called 'above_average' that is set to 1 if value is above the mean_price\n",
    "data['above_average'] = [1 if price > mean_price else 0 for price in data['price']]\n",
    "\n",
    "\n",
    "# split the data to train/val/test sets with 60%/20%/20% distribution\n",
    "df_full_train, df_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# set the y dataframe\n",
    "y_train = df_train.above_average.values\n",
    "y_val = df_val.above_average.values\n",
    "\n",
    "df_train.drop(['price', 'above_average'], axis=1, inplace=True)\n",
    "df_val.drop(['price', 'above_average'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# perform OHE on categorical data\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = df_train.to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "val_dict = df_val.to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n",
    "\n",
    "\n",
    "# Train a model with all the features\n",
    "model = LogisticRegression(solver='liblinear', C=10, max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "y_pred = model.predict(X_val)\n",
    "# mse = mean_squared_error(y_val, y_pred)\n",
    "# accuracy = 1 - (mse / np.var(y_val))\n",
    "accuracy = round(model.score(X_val, y_val), 3)\n",
    "print(f'The accuracy of the model with all features is {accuracy:.2%}')\n",
    "# print(f'The accuracy of the model with all features is {model.score(X_val, y_val):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude each feature from this set and train a model without it\n",
    "for feature in df_train.columns:\n",
    "    print(f'Removing {feature}')\n",
    "    train_new = df_train.drop(feature, axis=1)\n",
    "    val_new = df_val.drop(feature, axis=1)\n",
    "\n",
    "    # display(train_new.head())\n",
    "\n",
    "    # perform OHE on categorical data\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "\n",
    "    train_dict_new = train_new.to_dict(orient='records')\n",
    "    X_train_new = dv.fit_transform(train_dict_new)\n",
    "\n",
    "    val_dict = val_new.to_dict(orient='records')\n",
    "    X_val_new = dv.transform(val_dict)\n",
    "\n",
    "    model_new = LogisticRegression(solver='liblinear', C=10, max_iter=1000, random_state=42)\n",
    "    model_new.fit(X_train_new, y_train)\n",
    "    y_pred_new = model_new.predict(X_val_new)\n",
    "    # mse_new = mean_squared_error(y_val, y_pred_new)\n",
    "    # new_accuracy = 1 - (mse_new / np.var(y_val))\n",
    "    new_accuracy = round(model_new.score(X_val_new, y_val), 3)\n",
    "    diff = round((accuracy - new_accuracy), 6)\n",
    "    print(f'Number of features seen during fit = {model_new.n_features_in_}')\n",
    "    print(f'old {accuracy = }')\n",
    "    print(f'{new_accuracy = }')\n",
    "    print(f'The difference in accuracy after excluding {feature} is {diff:.2%}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Q: Which of these alphas leads to the best RMSE on the validation set?\n",
    "\n",
    "A: `0`\n",
    "\n",
    "\n",
    "### From the top part 2, prepare_X_y()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X_y():\n",
    "\n",
    "    # load the data\n",
    "    data = pd.read_csv('../data/data-hmwk-3.csv')\n",
    "\n",
    "    # identify features and target\n",
    "    features = ['make', 'model', 'year', 'engine_hp', 'engine_cylinders', \n",
    "                'transmission_type', 'vehicle_style',\n",
    "                'highway_mpg', 'city_mpg']\n",
    "    target = ['price']\n",
    "\n",
    "    # clean/prepare the data\n",
    "    data.columns = data.columns.str.replace(' ', '_').str.lower()\n",
    "    data.columns = data.columns.str.replace('msrp', 'price')\n",
    "\n",
    "    # use a subset for analysis\n",
    "    data = data[features+target]\n",
    "\n",
    "    # impute Nan/nulls with 0\n",
    "    data = data.fillna(0)\n",
    "\n",
    "    # split the data to train/val/test sets with 60%/20%/20% distribution\n",
    "    df_full_train, df_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n",
    "\n",
    "    # set the y dataframe\n",
    "    y_train = np.log1p(df_train.price.values)\n",
    "    y_val = np.log1p(df_val.price.values)\n",
    "\n",
    "    # remove target from df\n",
    "    del df_train['price']\n",
    "    del df_val['price']\n",
    "\n",
    "\n",
    "    # perform OHE on categorical data; made sparse due to non-convergence otherwise\n",
    "    dv = DictVectorizer(sparse=True)\n",
    "\n",
    "    train_dict = df_train.to_dict(orient='records')\n",
    "    X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "    val_dict = df_val.to_dict(orient='records')\n",
    "    X_val = dv.transform(val_dict)\n",
    "\n",
    "    # print(X.shape, y.shape)\n",
    "    return X_train, y_train, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning `alpha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "rmse_scores = {}\n",
    "alpha = [0, 0.01, 0.1, 1, 10]\n",
    "\n",
    "X_train, y_train, X_val, y_val = prepare_X_y()\n",
    "\n",
    "for num in alpha:\n",
    "    model = Ridge(alpha=num, solver='sag', random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    print(f'for alpha = {num}, rmse = {rmse:.6f}')\n",
    "    rmse_scores[num] = round(rmse, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(rmse_scores, indent=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
